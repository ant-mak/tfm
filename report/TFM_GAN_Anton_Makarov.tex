\documentclass[a4paper,12pt]{report}

\usepackage{graphicx}

\usepackage[english, spanish]{babel}
\usepackage[latin1]{inputenc}
\usepackage[nottoc]{tocbibind}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[linesnumbered, ruled, spanish]{algorithm2e}	
\usepackage{url}

\newtheorem{defn}{Definición}
\newtheorem{teorema}{Teorema}
\newtheorem{lema}{Lema}
\newtheorem{obs}{Observación}
\newtheorem{prop}{Proposición}

\newenvironment{abstractpage}
{\cleardoublepage\vspace*{\fill}\thispagestyle{empty}}
{\vfill\cleardoublepage}
\newenvironment{abstractnew}[1]
{\bigskip\selectlanguage{#1}%
	\begin{center}\bfseries\abstractname\end{center}}
{\par\bigskip}
		
\textheight=25cm \addtolength{\topmargin}{-2.5cm}

\begin{document}
\thispagestyle{empty}
\begin{center}

{\large\bf UNIVERSIDAD COMPLUTENSE DE MADRID} \\
{\bf\small FACULTAD DE CIENCIAS MATEMÁTICAS}\\[0.35cm]
{\large\bf UNIVERSIDAD POLITÉCNICA DE MADRID}\\
{\bf\small ESCUELA TÉCNICA SUPERIOR DE INGENIEROS DE TELECOMUNICACIÓN}\\[1.2cm]

{\bf MÁSTER EN TRATAMIENTO ESTADÍSTICO COMPUTACIONAL DE LA INFORMACIÓN}\\[1.2cm]
\mbox{}



\includegraphics[scale=1.5]{../images/cover/TECI-EscudoUCM} \hspace{2.5cm}
\includegraphics[scale=1.5]{../images/cover/TECI-EscudoUPM} \\[1.2cm]



{\large\bf TRABAJO DE FIN DE MÁSTER}\\[2cm]


{\large\bf Redes Generativas Antagónicas}\\[1cm]%Insertar título correspondiente

{\large\bf Antón Makarov Samusev}\\[1.5cm]%Insertar


{\bf Director}\\[0.4cm]
{\bf Francisco Javier Yáñez Gestoso} \\[2cm]

{\bf Madrid, 2019}
\end{center}

\newpage %Página blanco siguiente a la portada
\thispagestyle{empty}
\mbox{}\newpage

\begin{abstractpage}
	\begin{abstractnew}{spanish}
	Resumen
	\end{abstractnew}
	
	\begin{abstractnew}{english}
	Abstract
	\end{abstractnew}
\end{abstractpage}

\newpage %Página blanco siguiente a Resumen/Abstract
\thispagestyle{empty}
\mbox{}\newpage

\tableofcontents

\newpage %Página blanco siguiente a Índice
\thispagestyle{empty}
\mbox{}\newpage

\chapter{Introducción}
Introducción\footnote{Todo el código de este trabajo se puede encontrar en el repositorio de GitHub \url{https://github.com/ant-mak/tfm}, donde se incluyen las instrucciones para la reproducción de los resultados del proyecto.} \cite{goodfellow2014generative}, \cite{goodfellow2016nips}, \cite{radford2015unsupervised}, \cite{Goodfellow-et-al-2016}

\chapter{Preliminares}
En este capítulo realizaremos un breve resumen de los conceptos fundamentales del aprendizaje automático, introduciremos las definiciones necesarias para el desarrollo del resto del trabajo y daremos unas nociones básicas sobre aprendizaje profundo, haciendo hincapié en las redes convolucionales.
\section{Conceptos básicos}
\begin{defn}[Divergencia de Kullback-Leibler]
	KL div
\end{defn}
\begin{defn}[Divergencia de Jensen-Shannon]
	JS div
\end{defn}
\begin{defn}[Equilibrio de Nash]
	Eq. Nash
\end{defn}
\section{Redes neuronales}

\subsection{Aprendizaje profundo}
El aprendizaje profundo es un subgrupo de algoritmos de aprendizaje automático que buscan abstraer características de alto nivel presentes en los datos, utilizando generalmente, arquitecturas basadas en redes neuronales con una gran cantidad de capas.

En la actualidad son muy populares debido a su enorme potencial para resolver problemas complejos en ámbitos como la visión por computador o  el procesamiento del lenguaje natural, entre otros. Dicho potencial no ha podido ser aprovechado hasta hace muy poco debido a la escasez, por un lado, de conjuntos de datos adecuados y por otro, de recursos computacionales, ya que estos métodos requieren unas capacidades de cálculo y tamaños de conjuntos de datos muy superiores a otros métodos más tradicionales.
\subsection{Redes neuronales convolucionales}
Dado que nuestro objetivo es describir las redes generativas antagónicas y, más concretamente, utilizarlas para la generación de imágenes, es imprescindible introducir algunos de los conceptos más importantes de la familia de arquitecturas de aprendizaje profundo que están a la vanguardia en el campo de la visión por computador, las redes neuronales convolucionales (convolutional neural networks o CNNs en inglés).

Debemos sus primeros desarrollos a Kunihiko Fukushima, que en 1980 introdujo el neocognitrón \cite{fukushima1980neo}, que posteriormente sería tomado por Yann LeCun, que introdujo la mayoría de conceptos por los que conocemos este tipo de redes hoy en día.

La particularidad de las CNNs es que introducen una visión local del espacio. Pensemos en una imagen de un perro, parece razonable que intentemos identificar que efectivamente lo que vemos es un perro y no un gato fijándonos en pequeñas partes de la imagen como ojos, hocico, patas, orejas, etc. en lugar de todos los píxeles a la vez sin ningún tipo de relación espacial entre si. Es destacable que este tipo de filtros ya se hacían antes de las redes convolucionales, seleccionando regiones de interés manualmente. Sin embargo, las redes convolucionales van más allá, automatizando el proceso de extracción de características, aprendiendo durante el entrenamiento los filtros más idóneos.

Veamos ahora en detalle el funcionamiento de las redes convolucionales a la vez que desarrollamos los conceptos básicos que nos serán de utilidad a lo largo del resto del trabajo.
\begin{defn}
	Una imagen a color se puede definir como un tensor, con altura, y anchura el tamaño usual, por ejemplo $64 \times 64$ píxeles, a la que se añade otra dimensión, que llamaremos profundidad en la que se encuentran los canales de color rojo, verde y azul (Red Green Blue o RGB en inglés) con una cierta intensidad representada en un rango de $0$ a $255$. Mediante la combinación de dichos canales se forman las imágenes a color a las que estamos acostumbrados. En la Figura \ref{image_definition} tenemos una representación de esta idea.
\end{defn}
Ahora que tenemos la definición formal de imagen, comencemos con la descripción de la operación más importante, y la que da nombre a estas redes, la convolución. Tiene la función de extraer características de manera automática. Para ello, recorre el tensor de entrada de manera secuencial con una cuadrícula, aplicando a cada cuadrícula uno o varios filtros o \textbf{kernels}, de tamaño predeterminado y cuyos coeficientes son aprendidos por la red. La salida de una operación de convolución recibe el nombre de mapa de características. Para entender mejor el funcionamiento de la convolución veamos la Figura \ref{conv}. Tenemos a la derecha, representada en color azul una imagen de $5\times 5$ píxeles en $3$ canales, rojo, verde y azul a la que se añade un relleno o \textbf{padding} de ceros, representado en gris rodeando cada canal de la imagen, cuya función es recoger mejor la información sobre los bordes de la imagen y modular las dimensiones de salida. Recorremos la imagen mediante una cuadrícula $3 \times 3$ con un desplazamiento o \textbf{stride} de una posición. En la parte central de la imagen aparecen dos columnas con recuadros de color rojo. Dichos recuadros son los filtros. En este caso se utilizan dos, para obtener dos mapas de características distintos, representados a la izquierda en verde. Para obtener los mapas de características se realiza un producto elemento a elemento entre las cuadrículas de cada canal de color con las componentes de los filtros y finalmente se suman por posición.

En general se puede calcular la dimensión de salida de una capa convolucional mediante la siguiente fórmula:
\begin{equation*}
O = \frac{W-K+2P}{S}+1,
\end{equation*}
donde W es el tamaño de entrada, $K$ el tamaño del filtro, $P$ el padding y $S$ la longitud del paso. En nuestro ejemplo por tanto tendremos un tamaño de salida $O = \frac{5-3+2}{1}+1 = 5$ y, como hemos utilizado dos filtros, tendrá profundidad $2$, en notación común $5\times 5\times 2$.

\begin{obs}
	A la vista de la fórmula, el parámetro stride también influye en la dimensión de salida, cosa que utilizaremos esto extensivamente en las GANs.
\end{obs}

Una vez completada la fase de convolución, esta se suele acompañar de una reducción de muestreo o \textbf{pooling}, donde tomamos cada uno de los mapas de características obtenidos y lo dividimos en regiones. A los elementos de cada región les aplicamos una operación para comprimir la información que contienen. Las operaciones más utilizadas son el máximo, la media o el mínimo. En la Figura \ref{pool} nos encontramos ante un mapa de características de tamaño $4 \times 4$ que se ha dividido en $4$ regiones $2 \times 2$. A los valores de cada región se les aplica la función máximo, obteniendo la matriz de la izquierda, reduciendo así la dimensión del mapa de características.

Recientemente se ha incorporado una técnica para mejorar la estabilidad en el entrenamiento de redes convolucionales, llamada Batch Normalization \cite{ioffe2015batch}. La idea fundamental es, si normalizamos los datos antes de introducirlos en la capa de entrada para reducir las influencias de las magnitudes de las variables y ajustarlos a una distribución, ¿por qué no hacer lo mismo en todas las capas?

Dado que los algoritmos de optimización pueden deshacer la normalización que introduzcamos si con ello reducen el valor de la función objetivo, es necesario introducir dos parámetros adicionales $\gamma$ y $\beta$ que serán aprendidos por la red. En el algoritmo \ref{alg:batchnorm} mostramos un breve esquema del funcionamiento, para más detalles consultar \cite{ioffe2015batch}.

\begin{algorithm}[h]
	Calcular la media del batch: $\mu_B = \frac{1}{m} \sum_{i=1}^m x_i$\;
	Calcular la varianza del batch: $\sigma^2_b = \frac{1}{m} \sum_{i = 1}^m (x_i - \mu_B)^2$\;
	Normalizar: $\hat{x_i} = \frac{x_i - \mu_b}{\sqrt{\sigma_B}}$\;
	Escalar y trasladar: $y_i = \gamma x_i + \beta$\;
	\caption{Entrenamiento de una red generativa antagónica genérica.}
	\label{alg:batchnorm}
\end{algorithm}


\begin{defn}[ReLU]
	La función de activación unidad lineal rectificada (REctified Linear Unit o ReLU en inglés) se define como:
	\begin{equation*}
	f(x) = \max(0,x)
	\end{equation*}
	También será de utilidad la función LeakyReLU, definida como:
	\begin{equation*}
	f(x) =
	\begin{cases}
	x & \text{si $x>0$} \\
	0.01x & \text{en otro caso}
	\end{cases}
	\end{equation*}
\end{defn}
Estas funciones se utilizan en aprendizaje profundo en lugar de las sigmoides y tangentes hiperbólicas porque son más eficientes computacionalmente y por su capacidad de evitar el desvanecimiento del gradiente.
\begin{defn}
	El desvanecimiento del gradiente se produce cuando la función de pérdida se acerca demasiado a cero. Ocurre cuando se añaden muchas capas que utilizan ciertos tipos de funciones de activación. Por ejemplo la sigmoide, que, para cambios grandes en la entrada, produce cambios muy pequeños en la salida, es decir, con derivada casi nula. Dado que para el cálculo del gradiente se utiliza el algoritmo de retropropagación, dichos valores casi nulos se multiplican entre si, resultando un gradiente despreciable.
\end{defn}


\chapter{Redes generativas antagónicas}
En este capítulo describiremos las ideas principales, tanto teóricas como prácticas de las redes generativas antagónicas, generative adversarial networks o GANs en inglés. Fueron propuestas por primera vez por Ian Goodfellow en 2014 \cite{goodfellow2014generative}, mezclando conceptos de aprendizaje automático no supervisado, supervisado y teoría de juegos. Desde entonces han suscitado una actividad investigadora muy importante, con aplicaciones en prácticamente todos los campos relacionados con el aprendizaje automático. 
\section{Idea general}
El objetivo de las GANs, y en general de los modelos generativos, es aprender la distribución que siguen los datos, pudiendo obtener así, en última instancia, muestras de dicha distribución. En general, las distribuciones que queremos modelar son muy complejas. Supongamos que nuestro objetivo es tomar muestras de la distribución de imágenes de perros, o dicho de otro modo, generar fotos de perros que sean realistas pero que no existan en la realidad ni sean una mezcla de imágenes de nuestro conjunto de entrenamiento. Tenemos la seguridad de que la distribución es extremadamente intrincada, existen perros de distintos colores, tamaños, razas, etc. Este problema es el que van a tratar de atacar las GANs.

La idea fundamental y más novedosa detrás de las GANs es poner dos redes neuronales a competir entre sí. Una red, llamada generadora $(G)$, está dedicada a obtener imágenes a partir de ruido aleatorio con distribución $p_g(z)$, mientras que otra, llamada discriminadora $(D)$, trata de averiguar si la imagen es real o ficticia. Es frecuente ilustrar esta idea mediante la analogía de falsificadores de billetes que tratan de engañar a la policía. Los falsificadores empiezan dibujando billetes que no tienen nada que ver con los reales, intentando utilizarlos para realizar pagos, momento en el que son atrapados por la policía. Los falsificadores por tanto se dan cuenta de que están dibujando los billetes de manera incorrecta y modifican su técnica, mientras que la policía va aprendiendo a su vez a detectar mejor los billetes falsos. De este modo, a lo largo del entrenamiento se busca llegar a un equilibrio, en el que la policía no sepa discernir los billetes falsos de los verdaderos, obteniendo así los ladrones una falsificación realista. Una representación esquemática se puede ver en la Figura \ref{esquema}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{../images/report/scheme.png}
	\label{esquema}
	\caption{Diagrama conceptual de las redes generativas antagónicas.}
\end{figure}
Podemos traducir esta idea a términos matemáticos de la siguiente manera:

Para aprender la distribución $p_g$ del generador sobre los datos $x$, definimos una distribución sobre las variables de ruido de entrada $p_z(z)$. Mediante un perceptrón multicapa con parámetros $\theta_g$ definimos la función $G(z;\theta_g)$. Definimos también mediante otro perceptrón multicapa $D(x; \theta_d)$ la probabilidad de que $x$ es una muestra de los datos y no del generador. Entrenamos $D$ para que maximice la probabilidad de asignar la etiqueta correcta tanto a los ejemplos de entrenamiento como a los generados. Al mismo tiempo, entrenamos $G$ para que minimice $\log (1- D(G(z)))$. Es decir, tenemos el siguiente juego minimax:
\begin{equation}
\min_G \max_D V(D,G) = \mathbb{E} [\log D(x)] + \mathbb{E} [\log (1- D(G(z)))].
\end{equation}

En el Algoritmo \ref{alg:gan} describimos lo anterior de manera más concisa y damos una primera idea de cómo se lleva a cabo el entrenamiento.
\begin{algorithm}[h]
	\For{Iteraciones de entrenamiento}
	{
	Tomar muestra $(z^{(1)}, z^{(2)}, \ldots, z^{(m)})$ de tamaño $m$ de $p_g(z)$\;
	Tomar muestra $(x^{(1)}, x^{(2)}, \ldots, x^{(m)})$ de tamaño $m$ de $p_{d}(x)$\;
	Actualizar el discriminador ascendiendo su gradiente:
	$$
	\nabla_{\theta_d} \frac{1}{m}\sum_{i = 1}^{m}\left[\log D\left( x^{i}\right) + \log \left( 1 - D(G(z^{i}))\right) \right]
	$$
	
	Actualizar el generador ascendiendo su gradiente:
	$$
	\nabla_{\theta_g} \frac{1}{m}\sum_{i = 1}^{m} \log \left( 1 - D(G(z^{i}))\right)
	$$
	}
	\caption{Entrenamiento de una red generativa antagónica genérica.}
	\label{alg:gan}
\end{algorithm}
\section{Bases teóricas}

En esta sección realizaremos un análisis de la teoría que hay detrás de las redes generativas antagónicas. Mostraremos que el criterio de entrenamiento nos permite recuperar la distribución de los datos si damos a $G$ y $D$ capacidad suficiente. 

\begin{teorema}
	Para un generador $G$ fijo, el discriminador $D$ óptimo es:
	\begin{equation}
	D_G^*(x) = \frac{p_{datos}(x)}{p_{datos}(x) + p_g(x)}
	\end{equation}
\end{teorema}
\begin{proof}
	El discriminador busca maximizar su función de utilidad, dada por $V(G,D)$
	\begin{equation}
	\begin{aligned}
	V(G,D) &= \int_x p_{datos}(x) \log (D(x)) \mathrm{d}x + \int_z p_z(z) \log(1-D(g(z))) \mathrm{d}z\\
	&= \int_x p_{datos}(x) \log (D(x)) + p_g(x) \log (1-D(x)) \mathrm{d}x
	\end{aligned}
	\end{equation}
\end{proof}

\section{Otros modelos generativos}
Variational autoencoders, noise contrastive estimation etc.
\chapter{Generación de arte}
En esta sección nos vamos a apoyar sobre el artículo \cite{radford2015unsupervised} para implementar en \texttt{Python}, haciendo uso de \texttt{PyTorch} una red convolucional profunda generativa antagónica, con la cual vamos a tratar de generar imágenes de cuadros realistas.
\section{DCGAN}
Las redes convolucionales profundas generativas antagónicas (deep convolutional generative adversarial networks, DCGAN) son una extensión de las GANs tradicionales, que implementan una serie de recomendaciones para incrementar la estabilidad y obtener imágenes de mejor calidad. Los puntos más relevantes que se pueden extraer de \cite{radford2015unsupervised} son:
\begin{itemize}
	\item En el discriminador, utilizar convoluciones con stride en lugar de capas de pooling.
	\item En el generador, utilizar convoluciones fraccionales con stride en lugar de capas de pooling.
	\item Utilizar BatchNorm tanto para generador como discriminador.
	\item No utilizar capas fully connected.
	\item Utilizar funciones de activación ReLU en el generador, salvo para la última capa, en la que se propone usar $\tanh$.
	\item Utilizar funciones de activación LeakyReLU en el discriminador.
	\item Inicializar los pesos de ambas redes con una distribución normal.
\end{itemize}
RECORDAR QUE STRIDE Y POOL SON PARECIDOS Y DESCRIBIR LEAKYRELU.

El objetivo que nos hemos propuesto es la generación de cuadros realistas, para ello, es necesario un amplio conjunto de datos con cuadros de distintos artístas, épocas y estilos. Hemos encontrado en  una competición de Kaggle un conjunto de datos con las características deseadas, con más de 100000 imágenes, ocupando aproximadamente 49 GB. Una muestra de dichas imágenes se puede observar en la Figura \ref{img_reales}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.60]{../images/results/original_data.png}
	\label{img_reales}
	\caption{Imágenes del conjunto de datos.}
\end{figure}

\subsection{Preprocesado}
El conjunto de datos tenía algunos defectos, como por ejemplo imágenes descargadas incorrectamente o imágenes corruptas en el propio conjunto de datos. Se ha tomado la decisión de eliminar dichas imágenes. ya que son pocas y no hay una manera directa de recuperarlas.

Una vez limpio el conjunto de datos, es necesario realizar algunas transformaciones para que, posteriormente, nuestras redes neuronales puedan utilizar las imágenes. En primer lugar hemos homogeneizado los tamaños y las proporciones. Pasamos así de cuadros de todos los tamaños y de distintas formas a cuadrados de $64\times 64$ píxeles. Posteriormente, realizamos un recorte centrado y finalmente las cargamos como tensores.

\subsection{Arquitectura}
Siguiendo las recomendaciones del artículo \cite{radford2015unsupervised}, hemos decidido utilizar la arquitectura que se muestra en la Figura \ref{esqma}. También hemos inicializado los pesos con una distribución normal $\mathcal{N}()$ y hemos seleccionado un tamaño de batch de 128.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{../images/report/architecture_scheme.png}
	\label{esqma}
	\caption{Arquitectura de las redes (Cambiar, poner tamaños de entrada y salida).}
\end{figure}
La red se ha entrenado durante 30 épocas, utilizando el algoritmo de optimización Adam \cite{kingma2014adam} con una tasa de aprendizaje de $0.0002$ y $\beta = (0.5,0.999)$.
\subsection{Recursos}
Cabe destacar que ha sido imprescindible el uso de un ordenador con GPU compatible con CUDA, en concreto se ha utilizado una Nvidia Quadro P5000. El entrenamiento de principio a fin ha tardado aproximadamente 24 horas. También se han realizado pruebas con CPU en un portátil de prestaciones modestas y se ha observado que el rendimiento era unas 20 veces menor.
\section{Resultados}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.60]{../images/results/generated_data.png}
	\label{img_generadas}
	\caption{Imágenes generadas mediante la DCGAN implementada después de 30 épocas de entrenamiento.}
\end{figure}
\chapter{Conclusión}


\bibliography{../bibliography/tfmbib}
\bibliographystyle{abbrv}
\end{document}
